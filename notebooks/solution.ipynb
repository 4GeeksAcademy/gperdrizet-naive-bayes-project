{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier: Google Play Store reviews\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Python standard library imports\n",
    "import string\n",
    "from pathlib import Path\n",
    "\n",
    "# PyPI imports\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data loading\n",
    "\n",
    "### 1.1. Load data from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from the URL provided in the project tutorial\n",
    "data_df = pd.read_csv('https://raw.githubusercontent.com/4GeeksAcademy/naive-bayes-project-tutorial/main/playstore_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Save a local copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a directory for raw data\n",
    "Path('../data/raw').mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save a local copy of the raw data\n",
    "data_df.to_parquet('../data/raw/playstore_reviews.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Review length distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = data_df['review'].str.len().tolist()\n",
    "\n",
    "plt.title('Review length distribution')\n",
    "plt.xlabel('Characters')\n",
    "plt.ylabel('Reviews')\n",
    "plt.hist(lengths, bins=30, color='black')\n",
    "plt.show()\n",
    "\n",
    "print(f'Review length mean: {np.mean(lengths):.0f}')\n",
    "print(f'Review length min: {min(lengths):.0f}')\n",
    "print(f'Review length max: {max(lengths):.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Long reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df['review_length'] = lengths\n",
    "long_reviews = data_df[data_df['review_length'] > 600]\n",
    "\n",
    "print(long_reviews['review'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(long_reviews['review'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(long_reviews['review'].iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Short reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_length = 30\n",
    "\n",
    "short_reviews = data_df[data_df['review_length'] < short_length]\n",
    "short_reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Filter reviews by length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df[(data_df['review_length'] >= 10) & (data_df['review_length'] <= 600)]\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bag-of-words encoding\n",
    "\n",
    "### 3.1. Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the reviews\n",
    "reviews = data_df['review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the reviews\n",
    "reviews = reviews.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers\n",
    "reviews = reviews.str.replace(r'\\d+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "reviews = reviews.str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the text\n",
    "\n",
    "def lemmatize_text(text):\n",
    "\n",
    "    # Create a lemmatizer object\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokenize the text into words\n",
    "    words = nltk.word_tokenize(text)\n",
    "\n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Join the lemmatized words back into a string\n",
    "    lemmatized_text = ' '.join(lemmatized_words)\n",
    "\n",
    "    return lemmatized_text\n",
    "\n",
    "reviews = reviews.apply(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word count encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the count vectorizer and transform the reviews into word counts\n",
    "vector_model = CountVectorizer(stop_words=\"english\")\n",
    "word_counts = vector_model.fit_transform(reviews).toarray()\n",
    "\n",
    "print(f'Word count matrix has {word_counts.shape[0]} rows and {word_counts.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the count vectorizer and transform the reviews into word counts\n",
    "vector_model = CountVectorizer(stop_words=\"english\")\n",
    "word_counts = vector_model.fit_transform(reviews).toarray()\n",
    "\n",
    "print(f'Word count matrix has {word_counts.shape[0]} rows and {word_counts.shape[1]} columns')\n",
    "\n",
    "# Get the words from the vector model\n",
    "feature_names = vector_model.get_feature_names_out()\n",
    "\n",
    "# Convert the word counts back into a dataframe\n",
    "word_count_df = pd.DataFrame(word_counts, columns=feature_names)\n",
    "\n",
    "# # Add back the label\n",
    "# word_count_df['polarity'] = data_df['polarity']\n",
    "\n",
    "# Take a look...\n",
    "word_count_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Word count distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = word_count_df.sum().tolist()\n",
    "\n",
    "plt.title('Word count distribution')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Words')\n",
    "plt.hist(word_counts, bins=30, color='black')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_word_count_df = pd.DataFrame({\n",
    "    'word': word_count_df.columns,\n",
    "    'count': word_counts\n",
    "})\n",
    "\n",
    "trimmed_word_count_df = total_word_count_df[(total_word_count_df['count'] > 1) & (total_word_count_df['count'] < 200)]\n",
    "\n",
    "word_count_df = word_count_df[trimmed_word_count_df['word'].tolist()]\n",
    "word_count_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the label back\n",
    "word_count_df['polarity'] = data_df['polarity']\n",
    "\n",
    "# Train test split\n",
    "training_df, testing_df = train_test_split(word_count_df, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores={\n",
    "    'Model': [],\n",
    "    'Score': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    LogisticRegression(),\n",
    "    training_df.drop('polarity', axis=1),\n",
    "    training_df['polarity'],\n",
    "    cv=7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cross_val_scores['Model'].extend(['Logistic regression']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores*100)\n",
    "\n",
    "print(f'Cross-validation accuracy: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    MultinomialNB(),\n",
    "    training_df.drop('polarity', axis=1),\n",
    "    training_df['polarity'],\n",
    "    cv=7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cross_val_scores['Model'].extend(['Multinomial Naive Bayes']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores*100)\n",
    "\n",
    "print(f'Cross-validation accuracy: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    GaussianNB(),\n",
    "    training_df.drop('polarity', axis=1),\n",
    "    training_df['polarity'],\n",
    "    cv=7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cross_val_scores['Model'].extend(['Gaussian Naive Bayes']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores*100)\n",
    "\n",
    "print(f'Cross-validation accuracy: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Bernoulli Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(\n",
    "    BernoulliNB(),\n",
    "    training_df.drop('polarity', axis=1),\n",
    "    training_df['polarity'],\n",
    "    cv=7,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cross_val_scores['Model'].extend(['Bernoulli Naive Bayes']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores*100)\n",
    "\n",
    "print(f'Cross-validation accuracy: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5. Cross-validation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(pd.DataFrame.from_dict(cross_val_scores), x='Model', y='Score')\n",
    "plt.title('Model cross-validation performance comparison')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "model.fit(training_df.drop('polarity', axis=1), training_df['polarity'])\n",
    "\n",
    "testing_predictions = model.predict(testing_df.drop('polarity', axis=1))\n",
    "accuracy = accuracy_score(testing_predictions, testing_df['polarity'])*100\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm = confusion_matrix(testing_df['polarity'], testing_predictions, normalize='true')\n",
    "cm_disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "_ = cm_disp.plot()\n",
    "\n",
    "plt.title(f'Test set performance\\noverall accuracy: {accuracy:.1f}%')\n",
    "plt.xlabel('Predicted outcome')\n",
    "plt.ylabel('True outcome')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
